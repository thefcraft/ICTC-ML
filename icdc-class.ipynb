{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1700 entries, 0 to 1699\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ben     1700 non-null   object\n",
      " 1   guj     1700 non-null   object\n",
      " 2   hin     1700 non-null   object\n",
      " 3   kan     1700 non-null   object\n",
      " 4   mal     1700 non-null   object\n",
      " 5   ori     1700 non-null   object\n",
      " 6   pan     1700 non-null   object\n",
      " 7   tam     1700 non-null   object\n",
      " 8   tel     1700 non-null   object\n",
      " 9   urd     1700 non-null   object\n",
      " 10  eng     1700 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 146.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('icdc\\\\train.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [00:00<00:00, 2934.49it/s]\n"
     ]
    }
   ],
   "source": [
    "allTexts = ''\n",
    "for i in tqdm(range(df.__len__())):\n",
    "    allTexts += ''.join(df.iloc[i]).lower().replace('–','').replace('$','').replace('&','').replace('[','').replace(']',''\n",
    "                                            ).replace('“','').replace('”','').replace('=','').replace('৷','').replace('`','').replace('ؑ', '').replace('}',''\n",
    "                                            ).replace('-', '').replace('*', '').replace('^', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglish_res = Counter(allTexts)\n",
    "# sorted(list(dict(hinglish_res).items()), key = lambda x: x[1], reverse=True)\n",
    "charsVocab = list(dict(hinglish_res).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_NULL = '-'\n",
    "PAD_START = '*'\n",
    "PAD_END = '^'\n",
    "\n",
    "vocab = [PAD_NULL, PAD_START, PAD_END]+[i[0] for i in charsVocab]\n",
    "\n",
    "IDX_PAD_NULL = vocab.index(PAD_NULL)\n",
    "\n",
    "len(vocab), IDX_PAD_NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau#, StepLR, ExponentialLR\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1],\n",
       "        [14, 19],\n",
       "        [ 4, 13],\n",
       "        [ 4, 17],\n",
       "        [12,  3],\n",
       "        [ 0, 37],\n",
       "        [ 0, 37],\n",
       "        [ 0, 37],\n",
       "        [ 0, 14]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_extraToken(texts, startToken=True, endToken=True):\n",
    "    if startToken and endToken: return [PAD_START+text+PAD_END for text in texts]\n",
    "    elif startToken: return [PAD_START+text for text in texts]\n",
    "    elif endToken: return [text+PAD_END for text in texts]\n",
    "    else: return texts\n",
    "\n",
    "def remove_extraToken(texts:list[str])->list[str]:\n",
    "    return [text.lower().replace('–','').replace('$','').replace('&','').replace('[','').replace(']',''\n",
    "                                            ).replace('“','').replace('”','').replace('=','').replace('৷','').replace('`','').replace('ؑ', '').replace('}',''\n",
    "                                            ).replace(PAD_START, '').replace(PAD_END, '').replace(PAD_NULL, '')\n",
    "            for text in texts]\n",
    "\n",
    "def preprocesser(texts: list[str], prePadding=False, startToken=True, endToken=True, batch_first=False):\n",
    "    texts = add_extraToken(remove_extraToken(texts), startToken, endToken)\n",
    "    text_ints = [[vocab.index(c) for c in text if c in vocab] for text in texts]\n",
    "    # Apply pre-padding to each sequence\n",
    "    if prePadding:\n",
    "        max_length = max(len(seq) for seq in text_ints)\n",
    "        padded_seqs = pad_sequence([torch.cat([torch.tensor([IDX_PAD_NULL]*(max_length - len(seq)), dtype=torch.int64), torch.LongTensor(seq)]) for seq in text_ints], batch_first=True)\n",
    "    else:\n",
    "        padded_seqs = pad_sequence([torch.LongTensor(seq) for seq in text_ints], batch_first=True, padding_value=IDX_PAD_NULL)\n",
    "    \n",
    "    return padded_seqs if batch_first else padded_seqs.T\n",
    "\n",
    "\n",
    "preprocesser(['hiir', 'laksfffh'], startToken=True, endToken=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, batch_size=64):\n",
    "        dataset = []\n",
    "\n",
    "        for y, col in enumerate(df.columns):\n",
    "            for i in range(df[col].__len__()):\n",
    "                text = df[col].iloc[i].lower().replace('–','').replace('$','').replace('&','').replace('[','').replace(']',''\n",
    "                                            ).replace('“','').replace('”','').replace('=','').replace('৷','').replace('`','').replace('ؑ', '').replace('}',''\n",
    "                                            ).replace(PAD_START, '').replace(PAD_END, '').replace(PAD_NULL, '')\n",
    "                dataset.append((text, y))\n",
    "        \n",
    "        dataset.sort(key=lambda x: len(x[0]))\n",
    "        \n",
    "        self.batched = []\n",
    "        for i in range(0, len(dataset), batch_size): self.batched.append(self.custom_collate_fn(dataset[i:i+batch_size]))\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        x = []\n",
    "        y = []\n",
    "        for ix, iy in batch:\n",
    "            x.append(ix)\n",
    "            y.append(iy)\n",
    "        return preprocesser(x), F.one_hot(torch.tensor(y), num_classes=11).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single sequence and its label\n",
    "        return self.batched[idx]\n",
    "\n",
    "# Create a DataLoader with batch size 64\n",
    "custom_dataset = CustomDataset(batch_size=64)  # Create an instance of the custom dataset\n",
    "data_loader = DataLoader(custom_dataset, batch_size=1, shuffle=True)\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    sequences, labels = batch\n",
    "    sequences.squeeze_(0)\n",
    "    labels.squeeze_(0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, vocab_size, p=0, num_classes=11):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=p, bidirectional=False) \n",
    "        # self.fc1 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        # self.fc2 = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (sequencen x batch_size)\n",
    "        x = self.dropout(self.embedding(x)) # (sequencen x batch_size x embedding_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(x) # (sequencen x batch_size x hidden_size), ((num_layers x batch_size x hidden_size), (num_layers x batch_size x hidden_size))\n",
    "        return self.fc(outputs[-1])\n",
    "        # x = F.relu(self.fc1(outputs[-1]))\n",
    "        # return self.fc2(x)\n",
    "\n",
    "\n",
    "# Create an LSTM model\n",
    "# model = Encoder(50, 128, 2, vocab_size=len(vocab)).to(DEVICE)\n",
    "# x = sequences\n",
    "# y = labels\n",
    "# print(x.shape)\n",
    "# model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "LR = 0.001\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "P = 0.5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "TRAIN_SIZE = .8\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def accuracy(model, data_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Disable gradient computation during inference\n",
    "    for (sequences, labels) in data_loader: # test_loader\n",
    "        sequences = sequences.squeeze(0).to(DEVICE)\n",
    "        labels = labels.squeeze(0).to(DEVICE).argmax(dim=1)\n",
    "        # Forward pass\n",
    "        predicted = model(sequences).argmax(dim=1)\n",
    "            \n",
    "        # Count total number of labels\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Count number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    model.train()\n",
    "    # Calculate accuracy\n",
    "    return 100 * correct / total\n",
    "    # print('Accuracy: {:.2f}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader with batch size 64\n",
    "custom_dataset = CustomDataset(BATCH_SIZE)\n",
    "\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = Encoder(EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS, vocab_size=len(vocab), p=P, num_classes=11).to(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 1s] Train Epoch: [0/100] \tLoss: 420.80 Test Loss: 302.68\n",
      "[0m 3s] Train Epoch: [1/100] \tLoss: 296.14 Test Loss: 229.76\n",
      "[0m 5s] Train Epoch: [2/100] \tLoss: 233.17 Test Loss: 198.54\n",
      "[0m 6s] Train Epoch: [3/100] \tLoss: 203.94 Test Loss: 155.61\n",
      "[0m 7s] Train Epoch: [4/100] \tLoss: 174.66 Test Loss: 149.46\n",
      "[0m 9s] Train Epoch: [5/100] \tLoss: 156.74 Test Loss: 126.70\n",
      "[0m 10s] Train Epoch: [6/100] \tLoss: 147.91 Test Loss: 150.09\n",
      "[0m 12s] Train Epoch: [7/100] \tLoss: 147.43 Test Loss: 118.94\n",
      "[0m 14s] Train Epoch: [8/100] \tLoss: 125.21 Test Loss: 110.80\n",
      "[0m 15s] Train Epoch: [9/100] \tLoss: 109.31 Test Loss: 100.82\n",
      "[0m 17s] Train Epoch: [10/100] \tLoss: 104.15 Test Loss: 96.16\n",
      "[0m 18s] Train Epoch: [11/100] \tLoss: 93.10 Test Loss: 80.57\n",
      "[0m 20s] Train Epoch: [12/100] \tLoss: 84.94 Test Loss: 81.97\n",
      "[0m 22s] Train Epoch: [13/100] \tLoss: 83.85 Test Loss: 77.63\n",
      "[0m 23s] Train Epoch: [14/100] \tLoss: 76.23 Test Loss: 65.69\n",
      "[0m 25s] Train Epoch: [15/100] \tLoss: 72.83 Test Loss: 68.07\n",
      "[0m 26s] Train Epoch: [16/100] \tLoss: 65.80 Test Loss: 61.89\n",
      "[0m 28s] Train Epoch: [17/100] \tLoss: 62.15 Test Loss: 63.03\n",
      "[0m 30s] Train Epoch: [18/100] \tLoss: 59.16 Test Loss: 55.07\n",
      "[0m 32s] Train Epoch: [19/100] \tLoss: 58.52 Test Loss: 55.27\n",
      "[0m 33s] Train Epoch: [20/100] \tLoss: 51.69 Test Loss: 55.88\n",
      "[0m 35s] Train Epoch: [21/100] \tLoss: 49.50 Test Loss: 54.30\n",
      "[0m 36s] Train Epoch: [22/100] \tLoss: 47.60 Test Loss: 45.76\n",
      "[0m 38s] Train Epoch: [23/100] \tLoss: 43.25 Test Loss: 45.98\n",
      "[0m 39s] Train Epoch: [24/100] \tLoss: 38.47 Test Loss: 47.97\n",
      "[0m 41s] Train Epoch: [25/100] \tLoss: 39.61 Test Loss: 42.90\n",
      "[0m 42s] Train Epoch: [26/100] \tLoss: 35.93 Test Loss: 44.85\n",
      "[0m 44s] Train Epoch: [27/100] \tLoss: 34.26 Test Loss: 45.20\n",
      "[0m 46s] Train Epoch: [28/100] \tLoss: 32.39 Test Loss: 40.80\n",
      "[0m 48s] Train Epoch: [29/100] \tLoss: 31.87 Test Loss: 43.25\n",
      "[0m 49s] Train Epoch: [30/100] \tLoss: 32.15 Test Loss: 43.89\n",
      "[0m 51s] Train Epoch: [31/100] \tLoss: 29.99 Test Loss: 41.80\n",
      "[0m 52s] Train Epoch: [32/100] \tLoss: 27.62 Test Loss: 39.37\n",
      "[0m 54s] Train Epoch: [33/100] \tLoss: 25.67 Test Loss: 38.20\n",
      "[0m 56s] Train Epoch: [34/100] \tLoss: 25.64 Test Loss: 38.36\n",
      "[0m 57s] Train Epoch: [35/100] \tLoss: 24.18 Test Loss: 36.56\n",
      "[0m 59s] Train Epoch: [36/100] \tLoss: 23.82 Test Loss: 37.37\n",
      "[1m 1s] Train Epoch: [37/100] \tLoss: 22.60 Test Loss: 38.23\n",
      "[1m 2s] Train Epoch: [38/100] \tLoss: 21.25 Test Loss: 34.78\n",
      "[1m 4s] Train Epoch: [39/100] \tLoss: 19.74 Test Loss: 35.53\n",
      "[1m 5s] Train Epoch: [40/100] \tLoss: 20.51 Test Loss: 38.83\n",
      "[1m 7s] Train Epoch: [41/100] \tLoss: 19.86 Test Loss: 36.16\n",
      "[1m 9s] Train Epoch: [42/100] \tLoss: 18.70 Test Loss: 40.14\n",
      "[1m 10s] Train Epoch: [43/100] \tLoss: 17.04 Test Loss: 35.36\n",
      "[1m 12s] Train Epoch: [44/100] \tLoss: 17.63 Test Loss: 37.23\n",
      "[1m 13s] Train Epoch: [45/100] \tLoss: 15.66 Test Loss: 35.03\n",
      "[1m 14s] Train Epoch: [46/100] \tLoss: 16.50 Test Loss: 36.85\n",
      "[1m 16s] Train Epoch: [47/100] \tLoss: 16.06 Test Loss: 36.93\n",
      "[1m 18s] Train Epoch: [48/100] \tLoss: 17.42 Test Loss: 36.00\n",
      "[1m 19s] Train Epoch: [49/100] \tLoss: 14.81 Test Loss: 34.85\n",
      "[1m 21s] Train Epoch: [50/100] \tLoss: 12.98 Test Loss: 33.79\n",
      "[1m 23s] Train Epoch: [51/100] \tLoss: 10.76 Test Loss: 33.94\n",
      "[1m 25s] Train Epoch: [52/100] \tLoss: 10.72 Test Loss: 34.12\n",
      "[1m 28s] Train Epoch: [53/100] \tLoss: 10.05 Test Loss: 34.09\n",
      "[1m 30s] Train Epoch: [54/100] \tLoss: 9.54 Test Loss: 34.72\n",
      "[1m 33s] Train Epoch: [55/100] \tLoss: 8.96 Test Loss: 34.41\n",
      "[1m 35s] Train Epoch: [56/100] \tLoss: 8.93 Test Loss: 35.10\n",
      "[1m 36s] Train Epoch: [57/100] \tLoss: 9.08 Test Loss: 34.02\n",
      "[1m 38s] Train Epoch: [58/100] \tLoss: 9.40 Test Loss: 33.75\n",
      "[1m 39s] Train Epoch: [59/100] \tLoss: 8.29 Test Loss: 34.33\n",
      "[1m 41s] Train Epoch: [60/100] \tLoss: 8.34 Test Loss: 35.18\n",
      "[1m 42s] Train Epoch: [61/100] \tLoss: 8.75 Test Loss: 35.47\n",
      "[1m 44s] Train Epoch: [62/100] \tLoss: 8.28 Test Loss: 34.55\n",
      "[1m 45s] Train Epoch: [63/100] \tLoss: 7.86 Test Loss: 34.96\n",
      "[1m 47s] Train Epoch: [64/100] \tLoss: 8.73 Test Loss: 34.55\n",
      "[1m 49s] Train Epoch: [65/100] \tLoss: 8.49 Test Loss: 34.10\n",
      "[1m 50s] Train Epoch: [66/100] \tLoss: 8.14 Test Loss: 34.31\n",
      "[1m 52s] Train Epoch: [67/100] \tLoss: 7.43 Test Loss: 34.54\n",
      "[1m 53s] Train Epoch: [68/100] \tLoss: 7.21 Test Loss: 34.60\n",
      "[1m 55s] Train Epoch: [69/100] \tLoss: 7.19 Test Loss: 35.39\n",
      "[1m 56s] Train Epoch: [70/100] \tLoss: 7.00 Test Loss: 35.05\n",
      "[1m 58s] Train Epoch: [71/100] \tLoss: 7.00 Test Loss: 35.19\n",
      "[1m 59s] Train Epoch: [72/100] \tLoss: 7.22 Test Loss: 35.07\n",
      "[2m 1s] Train Epoch: [73/100] \tLoss: 7.66 Test Loss: 34.95\n",
      "[2m 2s] Train Epoch: [74/100] \tLoss: 7.50 Test Loss: 34.92\n",
      "[2m 4s] Train Epoch: [75/100] \tLoss: 7.25 Test Loss: 34.95\n",
      "[2m 5s] Train Epoch: [76/100] \tLoss: 6.80 Test Loss: 34.92\n",
      "[2m 7s] Train Epoch: [77/100] \tLoss: 7.13 Test Loss: 35.17\n",
      "[2m 8s] Train Epoch: [78/100] \tLoss: 7.27 Test Loss: 34.94\n",
      "[2m 9s] Train Epoch: [79/100] \tLoss: 7.28 Test Loss: 34.87\n",
      "[2m 11s] Train Epoch: [80/100] \tLoss: 7.26 Test Loss: 34.93\n",
      "[2m 12s] Train Epoch: [81/100] \tLoss: 7.23 Test Loss: 34.94\n",
      "[2m 13s] Train Epoch: [82/100] \tLoss: 6.79 Test Loss: 34.94\n",
      "[2m 15s] Train Epoch: [83/100] \tLoss: 7.18 Test Loss: 34.91\n",
      "[2m 16s] Train Epoch: [84/100] \tLoss: 7.44 Test Loss: 34.91\n",
      "[2m 17s] Train Epoch: [85/100] \tLoss: 7.13 Test Loss: 34.89\n",
      "[2m 19s] Train Epoch: [86/100] \tLoss: 7.11 Test Loss: 34.89\n",
      "[2m 20s] Train Epoch: [87/100] \tLoss: 6.61 Test Loss: 34.87\n",
      "[2m 22s] Train Epoch: [88/100] \tLoss: 7.02 Test Loss: 34.89\n",
      "[2m 23s] Train Epoch: [89/100] \tLoss: 6.88 Test Loss: 34.90\n",
      "[2m 24s] Train Epoch: [90/100] \tLoss: 6.68 Test Loss: 34.90\n",
      "[2m 26s] Train Epoch: [91/100] \tLoss: 6.78 Test Loss: 34.89\n",
      "[2m 27s] Train Epoch: [92/100] \tLoss: 6.74 Test Loss: 34.88\n",
      "[2m 28s] Train Epoch: [93/100] \tLoss: 7.14 Test Loss: 34.88\n",
      "[2m 30s] Train Epoch: [94/100] \tLoss: 7.42 Test Loss: 34.89\n",
      "[2m 31s] Train Epoch: [95/100] \tLoss: 7.55 Test Loss: 34.89\n",
      "[2m 32s] Train Epoch: [96/100] \tLoss: 6.81 Test Loss: 34.89\n",
      "[2m 34s] Train Epoch: [97/100] \tLoss: 7.35 Test Loss: 34.89\n",
      "[2m 35s] Train Epoch: [98/100] \tLoss: 6.95 Test Loss: 34.89\n",
      "[2m 37s] Train Epoch: [99/100] \tLoss: 6.70 Test Loss: 34.89\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    # Iterate through the DataLoader\n",
    "    model.train()\n",
    "    for (sequences, labels) in train_loader:\n",
    "        sequences = sequences.squeeze(0).to(DEVICE)\n",
    "        labels = labels.squeeze(0).to(DEVICE)\n",
    "        \n",
    "        output = model(sequences)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation phase\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (sequences, labels) in test_loader:\n",
    "            sequences = sequences.squeeze(0).to(DEVICE)\n",
    "            labels = labels.squeeze(0).to(DEVICE)\n",
    "        \n",
    "            output = model(sequences)\n",
    "        \n",
    "            loss = criterion(output, labels)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    print('[{}] Train Epoch: [{}/{}] \\tLoss: {:.2f} Test Loss: {:.2f}'.format(\n",
    "            time_since(start), epoch, EPOCHS,\n",
    "            total_loss, valid_loss*len(train_loader)/len(test_loader)))\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.95%\n",
      "Test Accuracy: 96.19%\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: {:.2f}%'.format(accuracy(model, train_loader)))\n",
    "print('Test Accuracy: {:.2f}%'.format(accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'models_icdc\\\\last.model.pth')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = Encoder(50, 128, 2, vocab_size=64, p=.5, num_classes=11)\n",
    "loaded_model.load_state_dict(torch.load('models_icdc\\\\last.model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ben']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(model, texts:list[str]|str):\n",
    "    if isinstance(texts, str): texts = [texts]\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        outputs =  model(preprocesser(texts)).argmax(dim=1).cpu().numpy()\n",
    "        for i in outputs:\n",
    "            results.append(['ben', 'guj', 'hin', 'kan', 'mal', 'ori', 'pan', 'tam', 'tel', 'urd', 'eng'][i])\n",
    "    return results\n",
    "inference(loaded_model, 'alute masala makhie, fetano basena chubie nie dubo tele bhaja yatakshan na bhalo kare bhaja hachche, tiri kara has maharashtrer ei suswadu o janapriya khavarer pad.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = []\n",
    "for i in pd.read_csv('icdc\\\\language classification\\\\classification-submission.csv').text:\n",
    "    lang = inference(loaded_model, i)[0]\n",
    "    mine.append((i, lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "blang = inference(loaded_model, [i for i in pd.read_csv('icdc\\\\language classification\\\\classification-submission.csv').text])\n",
    "mine = [(i0, i1, i2) for (i0, i1), i2 in zip(mine, blang)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = pd.DataFrame(mine, columns=['text', 'lang', 'langbatched'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.to_csv('icdc\\\\language classification\\\\classification-submission-mine.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "o['bothequal'] = (o.lang == o.langbatched)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
